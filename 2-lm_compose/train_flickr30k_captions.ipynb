{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## library\n",
    "# GPU selection\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# minimum\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Flatten, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Masking, Dropout\n",
    "from tensorflow.keras.layers import Input, Lambda, RepeatVector, Reshape\n",
    "from tensorflow.keras.layers import TimeDistributed, Concatenate, Dot, Add\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the pre-processed data\n",
    "flickr30k_images = np.load('/home/librarian/corpus/flickr30k/flickr30k_vectors49-resnet50.npy', allow_pickle=True)\n",
    "\n",
    "test_image_ids    = np.load('preprocessed_flicker30k/test_image_ids.npy', allow_pickle=True)\n",
    "train_image_ids   = np.load('preprocessed_flicker30k/train_image_ids.npy', allow_pickle=True)\n",
    "image_ids_indices = np.load('preprocessed_flicker30k/image_ids_indices.npy', allow_pickle=True)[None][0]\n",
    "ix2word           = np.load('preprocessed_flicker30k/ix2word.npy', allow_pickle=True)[None][0]\n",
    "captions          = np.load('preprocessed_flicker30k/captions.npy', allow_pickle=True)\n",
    "max_len           = len(captions[0,1]) - 2\n",
    "\n",
    "def ixs2sent(ixs):\n",
    "    return [ix2word[ix] for ix in ixs if ix2word[ix] != '<pad/>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training and testing\n",
    "# train\n",
    "X_img_indices_train = np.array([\n",
    "    image_ids_indices[image_id]\n",
    "    for image_id, sent in captions\n",
    "    if image_id in train_image_ids\n",
    "])\n",
    "\n",
    "X_sents_train = np.array([\n",
    "    sent\n",
    "    for image_id, sent in captions\n",
    "    if image_id in train_image_ids\n",
    "])\n",
    "\n",
    "# test\n",
    "X_img_indices_test = np.array([\n",
    "    image_ids_indices[image_id]\n",
    "    for image_id, sent in captions\n",
    "    if image_id in test_image_ids\n",
    "])\n",
    "\n",
    "X_sents_test = np.array([\n",
    "    sent\n",
    "    for image_id, sent in captions\n",
    "    if image_id in test_image_ids\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_generator(batch_size=32, is_train=True):\n",
    "    if is_train:\n",
    "        X_img_indices = X_img_indices_train\n",
    "        X_sents = X_sents_train\n",
    "    else:\n",
    "        X_img_indices = X_img_indices_test\n",
    "        X_sents = X_sents_test\n",
    "\n",
    "    X_indices = np.arange(len(X_img_indices))\n",
    "    steps_per_epoch = int(len(X_indices)/batch_size)\n",
    "    \n",
    "    while True:\n",
    "        # shuffle \n",
    "        np.random.shuffle(X_indices)\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            sents = X_sents[X_indices[step*batch_size:(step+1)*batch_size]]\n",
    "            img_indices = X_img_indices[X_indices[step*batch_size:(step+1)*batch_size]]\n",
    "            yield ([sents[:, :-1], flickr30k_images[img_indices]], np.expand_dims(sents[:, 1:], 2))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # dimensionalities:\n",
    "    emb_size = 128\n",
    "    vfs_size = 128\n",
    "    regions_size = 7 * 7\n",
    "    visual_feature_size = 2048 # resnet50\n",
    "        \n",
    "    ### fine tune visual features\n",
    "    def mlp_vision(x): \n",
    "        x = Dense(vfs_size, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    ### repeat the image vector \n",
    "    def feature_fusion(x, max_len=max_len):\n",
    "        return K.concatenate([\n",
    "            x[0],\n",
    "            K.repeat_elements(K.expand_dims(x[1], 1), max_len+1, 1),\n",
    "        ], 2)\n",
    "\n",
    "    ### how to apply attention sequentially on image:\n",
    "    def apply_attention(x, max_len=max_len):\n",
    "        a, vf0 = x\n",
    "        vf0_ = K.repeat_elements(K.expand_dims(vf0, 1), max_len+1, 1)\n",
    "        \n",
    "        return K.sum(K.expand_dims(a, 3) * vf0_, 2)\n",
    "    \n",
    "    ### simple decoder model\n",
    "    ## inputs\n",
    "    # word embeddings\n",
    "    delayed_sentence = Input(shape=[max_len+1])\n",
    "    e_t  = Embedding(len(ix2word), emb_size)(delayed_sentence)\n",
    "    e_t  = Dropout(0.1)(e_t)\n",
    "    \n",
    "    # visual features\n",
    "    visual_features  = Input(shape=[regions_size, visual_feature_size]) \n",
    "    c   = mlp_vision(visual_features) \n",
    "    \n",
    "    # average visual features over all regions\n",
    "    c_g = GlobalAveragePooling2D()(Reshape([7 , 7, vfs_size])(c))\n",
    "    c_g = Flatten()(c_g)\n",
    "    \n",
    "    # fusing two modalities\n",
    "    ec_t = Lambda(feature_fusion)([e_t, c_g])\n",
    "    \n",
    "    # LSTM-language model\n",
    "    h_t  = LSTM(emb_size, dropout=0.1, return_sequences=True)(ec_t)\n",
    "    \n",
    "    # fusing two modalities again!\n",
    "    # but use spatial attention here.\n",
    "    # att 1\n",
    "    #_hc_t = Lambda(feature_fusion)([h_t, c_g])\n",
    "    #z_t   = Dense(emb_size, activation='tanh')(_hc_t)\n",
    "    #a_t   = Dense(regions_size, activation='softmax')(z_t)\n",
    "    #c_t   = Lambda(apply_attention)([a_t, c])\n",
    "    #final = Concatenate()([h_t, c_t])\n",
    "    \n",
    "    # simple fuse\n",
    "    #_hc_t = Lambda(feature_fusion)([h_t, c_g])\n",
    "    #final = Dense(emb_size, activation='relu')(_hc_t)\n",
    "    \n",
    "    # no extra fuse\n",
    "    final = h_t\n",
    "    \n",
    "    out   = Dense(len(ix2word), activation='softmax')(final)\n",
    "    model = Model([delayed_sentence, visual_features], out)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_79 (InputLayer)           (None, 49, 2048)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_104 (Dense)               (None, 49, 128)      262272      input_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_78 (InputLayer)           (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_37 (Reshape)            (None, 7, 7, 128)    0           dense_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_37 (Embedding)        (None, 26, 128)      2488448     input_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_18 (Gl (None, 128)          0           reshape_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 26, 128)      0           embedding_37[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_37 (Flatten)            (None, 128)          0           global_average_pooling2d_18[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 26, 256)      0           dropout_16[0][0]                 \n",
      "                                                                 flatten_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                  (None, 26, 128)      197120      lambda_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_105 (Dense)               (None, 26, 19441)    2507889     lstm_27[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,455,729\n",
      "Trainable params: 5,455,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 82ms/step - loss: 1.9345\n",
      "2369/2369 [==============================] - 187s 79ms/step - loss: 2.8373 - val_loss: 1.9345\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 1.7496\n",
      "1184/1184 [==============================] - 159s 134ms/step - loss: 1.8643 - val_loss: 1.7496\n",
      "Epoch 1/16\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 1.7019\n",
      "296/296 [==============================] - 115s 388ms/step - loss: 1.7429 - val_loss: 1.7019\n",
      "Epoch 2/16\n",
      "3/3 [==============================] - 1s 277ms/step - loss: 1.6948\n",
      "296/296 [==============================] - 115s 389ms/step - loss: 1.7065 - val_loss: 1.6948\n",
      "Epoch 3/16\n",
      "3/3 [==============================] - 1s 257ms/step - loss: 1.6490\n",
      "296/296 [==============================] - 114s 385ms/step - loss: 1.6730 - val_loss: 1.6490\n",
      "Epoch 4/16\n",
      "3/3 [==============================] - 1s 261ms/step - loss: 1.6209\n",
      "296/296 [==============================] - 114s 385ms/step - loss: 1.6421 - val_loss: 1.6209\n",
      "Epoch 5/16\n",
      "3/3 [==============================] - 1s 268ms/step - loss: 1.6237\n",
      "296/296 [==============================] - 115s 387ms/step - loss: 1.6134 - val_loss: 1.6237\n",
      "Epoch 6/16\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 1.5995\n",
      "296/296 [==============================] - 115s 387ms/step - loss: 1.5868 - val_loss: 1.5995\n",
      "Epoch 7/16\n",
      "3/3 [==============================] - 1s 261ms/step - loss: 1.5892\n",
      "296/296 [==============================] - 114s 386ms/step - loss: 1.5628 - val_loss: 1.5892\n",
      "Epoch 8/16\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 1.5709\n",
      "296/296 [==============================] - 114s 383ms/step - loss: 1.5411 - val_loss: 1.5709\n",
      "Epoch 9/16\n",
      "3/3 [==============================] - 1s 257ms/step - loss: 1.5505\n",
      "296/296 [==============================] - 114s 385ms/step - loss: 1.5213 - val_loss: 1.5505\n",
      "Epoch 10/16\n",
      "3/3 [==============================] - 1s 272ms/step - loss: 1.5617\n",
      "296/296 [==============================] - 114s 384ms/step - loss: 1.5030 - val_loss: 1.5617\n",
      "Epoch 11/16\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 1.5565\n",
      "296/296 [==============================] - 114s 385ms/step - loss: 1.4860 - val_loss: 1.5565\n",
      "Epoch 12/16\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 1.5381\n",
      "296/296 [==============================] - 114s 386ms/step - loss: 1.4705 - val_loss: 1.5381\n",
      "Epoch 13/16\n",
      "3/3 [==============================] - 1s 268ms/step - loss: 1.5350\n",
      "296/296 [==============================] - 114s 384ms/step - loss: 1.4561 - val_loss: 1.5350\n",
      "Epoch 14/16\n",
      "3/3 [==============================] - 1s 266ms/step - loss: 1.5477\n",
      "296/296 [==============================] - 115s 387ms/step - loss: 1.4428 - val_loss: 1.5477\n",
      "Epoch 15/16\n",
      "3/3 [==============================] - 1s 263ms/step - loss: 1.5244\n",
      "296/296 [==============================] - 114s 384ms/step - loss: 1.4305 - val_loss: 1.5244\n",
      "Epoch 16/16\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 1.5396\n",
      "296/296 [==============================] - 114s 385ms/step - loss: 1.4189 - val_loss: 1.5396\n"
     ]
    }
   ],
   "source": [
    "for epochs, batch_size in [(1, 64), (1, 128), (16, 512)]:\n",
    "    h = model.fit_generator(\n",
    "        generator=X_generator(batch_size=batch_size, is_train=True), \n",
    "        steps_per_epoch=int(len(X_sents_train)/batch_size), \n",
    "        validation_data=X_generator(batch_size=batch_size, is_train=False),\n",
    "        validation_steps=int(len(X_sents_test)/batch_size),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    history.append(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"simple_caption_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
